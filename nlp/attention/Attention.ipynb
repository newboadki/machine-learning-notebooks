{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8f09f2",
   "metadata": {},
   "source": [
    "**Table of contents**\n",
    "\n",
    "* [Attention](#1)\n",
    "    * [Definition](#1_1)\n",
    "    * [Motivation](#1_2)\n",
    "    * [General idea behind attention](#1_3)\n",
    "    * [Dot product as a measure of similarity](#1_4)\n",
    "* [Implementations](#2)\n",
    "    * [Bhadanau's](#2_1) [Bhadanau, et al (2014)](https://arxiv.org/abs/1409.0473)\n",
    "        * [Alignment scores](#2_1_1)\n",
    "        * [Create the inputs](#2_1_2)\n",
    "        * [First Layer: activations](#2_1_3)\n",
    "        * [Second Layer: scores](#2_1_4)\n",
    "        * [Calculate weights: Softmax function](#2_1_5)\n",
    "        * [Apply the weights to the encoder's hidden states](#2_1_6)\n",
    "        * [Code](#2_1_7)\n",
    "    * [Scaled dot-product attention](#2_2)\n",
    "        * [Code](#2_2_1)\n",
    "        * [Visualizing how the attention relates similar words from inputs and outputs](#2_2_2)    \n",
    "            * [Aligned vectors](#2_2_2_1)\n",
    "            * [Calculating embedding matrices for our concrete inputs and outputs](#2_2_2_2)\n",
    "            * [Code](#2_2_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6eee1",
   "metadata": {},
   "source": [
    "# Attention <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "## Definition <a class=\"anchor\" id=\"1_1\"></a>\n",
    "Attention is a mechanism that let's the model focus on specific parts of the input sequence when generating the next output. More concretely, since we are training a model, attention will help the model focus on the most relevant parts (according to the problem at hand) of the input when generating the next outpput.\n",
    "\n",
    "Attention is not exlcusive to _seq2seq_ models, neither is it to _NLP_. The rest of the document will focus on _NLP_ applications though.\n",
    "\n",
    "## Motivation <a class=\"anchor\" id=\"1_2\"></a>\n",
    "The attention mechanism was designed to address the information bottleneck problem suffered by _seq2seq_ models , particularly, with long input sequences.\n",
    "\n",
    "In the original _seq2seq_ networks with an encoder-decoder architecture, the encoder would only pass the last step's hidden state vector to the decoder. This means that a single hidden vector was used to encode every other hidden vector from previous steps. This resulted in low performarce as the size of the input sequences increased.\n",
    "\n",
    "<img src=\"./images/information-bottleneck.PNG\" alt=\"Information bottleneck\" width=\"80%\" />\n",
    "\n",
    "## General idea behind attention<a class=\"anchor\" id=\"1_3\"></a>\n",
    "Attention proposes to use all of the encoder's hidden state vectors together with the decoder's hidden state from the previous output step to form a weighted sum that gives more importance (weight) to the most relevant encoder hidden state vectors.\n",
    "    \n",
    "<img src=\"./images/attention-hidden-states.PNG\" alt=\"Attention combines encoder and decoder states into a context\" width=\"80%\" />    \n",
    "\n",
    "\n",
    "At least, the two attention mechanisms discussed in this article follow the next ideas:\n",
    "- Calculate a measure of similarity between inputs and outputs of the model, called Aligment.\n",
    "- Calculate a set of scores for each of those alignments. Scores are values between 0 and 1.\n",
    "- Use the scores as weights to weight the inputs so that the most relevant ones have higher values.\n",
    "- Sum the weighted inputs to form a context vector that gets passed to the decoder.\n",
    "\n",
    "<img src=\"./images/attention-general-steps.PNG\" alt=\"Attention's general steps\" width=\"80%\" />\n",
    "\n",
    "## Dot product as a measure of similarity<a class=\"anchor\" id=\"1_4\"></a>\n",
    "Both types of attention mechanisms discussed here use the dot product of matrices as a measure of similarity.\n",
    "\n",
    "$$\n",
    "\\large V路W = \\lVert V \\rVert路\\lVert W \\rVert \\cos(\\alpha)\n",
    "$$\n",
    "\n",
    "By definition, the dot product of orthogonal vectors is zero. And the more they go in the same direction the larger it is. The sign indicate same or opposite directions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234c835",
   "metadata": {},
   "source": [
    "# Implementations<a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "## Bhadanau's Attention:<a class=\"anchor\" id=\"2_1\"></a>\n",
    "The goal of the attention layer is to produce a context vector that contains the relevant information from the encoder's state. It consists of the following stages:\n",
    "- Calculate the alignments using a feedforward neural network\n",
    "- Apply a softmax function to the alignments to calculate some weights\n",
    "- Use the previous weights to scale the encoder's hidden vectors\n",
    "\n",
    "In this approach, the alignments are learnt using a neural network. Therefore, some weights are learnt to maximize the similarity measure between the encoder's hidden state vectors and the decoder's previous state hidden vector.\n",
    "\n",
    "### Alignment scores<a class=\"anchor\" id=\"2_1_1\"></a>\n",
    "This is a measure of similarity between the decoder hidden state and each encoder hidden state. The operation is designed in the the paper like below.\n",
    "\n",
    "$$\n",
    "\\large e_{ij} = v_a^\\top \\tanh{\\left(W_a s_{i-1} + U_a h_j\\right)}\n",
    "$$\n",
    "\n",
    "- $W_a \\in \\mathbb{R}^{n\\times m}$, $U_a \\in \\mathbb{R}^{n \\times m}$, and $v_a \\in \\mathbb{R}^m$\n",
    "are the weight matrices. \n",
    "- $n$ is the hidden state size. \n",
    "\n",
    "e_ij is a score of how well the inputs around j match the expected output at i. It is learnable. In practice, this is implemented as a feedforward neural network with two layers, where $m$ is the size of the layers in the alignment network.\n",
    "- Create the inputs, concatenating the encoder hidden vectors with the decoder's previous step hidden vector repeated.\n",
    "- Calculate the first layer of the FNN: the activations\n",
    "- Calculate the second layer: the scores.\n",
    "\n",
    "### Create the inputs<a class=\"anchor\" id=\"2_1_2\"></a>\n",
    "s_i-1 is repearted along axis 0 (rows) to form a matrix that can be concatenated with the hidden states along axis 1 (columns).\n",
    "\n",
    "$$\n",
    "\\mathbf{Inputs}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\cdots & h_1    & \\cdots \\\\\n",
    "\\cdots & h_2    & \\cdots \\\\\n",
    "\\cdots & h_3    & \\cdots \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "\\cdots & h_n    & \\cdots \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "\\cdots & s_{i-1} & \\cdots \\\\\n",
    "\\cdots & s_{i-1} & \\cdots \\\\\n",
    "\\cdots & s_{i-1} & \\cdots \\\\\n",
    "\\vdots & \\vdots  & \\vdots \\\\\n",
    "\\cdots & s_{i-1} & \\cdots \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- The encoder's hidden state's shape is (n, m)\n",
    "- The decoder's hidden state's shape is (n, m)\n",
    "- Inputs' shape is (n, 2*m). We are concatenating, not multiplicating.\n",
    "\n",
    "### First layer: activations <a class=\"anchor\" id=\"2_1_3\"></a>\n",
    "$$\n",
    "\\mathbf{scores}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\cdots & \\cdots    & \\cdots \\\\\n",
    "\\cdots & \\cdots    & \\cdots \\\\\n",
    "\\cdots & Inputs    & \\cdots \\\\\n",
    "\\vdots & \\vdots    & \\vdots \\\\\n",
    "\\cdots & \\cdots    & \\cdots \\\\\n",
    "\\end{bmatrix}\n",
    "路\n",
    "\\begin{bmatrix}\n",
    "\\cdots & \\cdots    & \\cdots \\\\\n",
    "\\cdots & \\cdots    & \\cdots \\\\\n",
    "\\cdots & Layer 1    & \\cdots \\\\\n",
    "\\vdots & \\vdots    & \\vdots \\\\\n",
    "\\cdots & \\cdots    & \\cdots \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Inputs' shape is (n, 2*m)\n",
    "- Layer_1's shape is (2*m, attention_size)\n",
    "- Activations' shape is (n, attention_size)\n",
    "\n",
    "\n",
    "### Second layer: scores <a class=\"anchor\" id=\"2_1_4\"></a>\n",
    "This steps produces score values, one per encoder's hidden state vector.\n",
    "$$\n",
    "\\mathbf{activations}\n",
    "=\n",
    "\\mathbf{tanh}\n",
    "\\begin{bmatrix}\n",
    "\\cdots & \\cdots    & \\cdots \\\\\n",
    "\\cdots & \\cdots    & \\cdots \\\\\n",
    "\\cdots & activations    & \\cdots \\\\\n",
    "\\vdots & \\vdots    & \\vdots \\\\\n",
    "\\cdots & \\cdots    & \\cdots \\\\\n",
    "\\end{bmatrix}\n",
    "路\n",
    "\\begin{bmatrix}\n",
    "\\vdots \\\\\n",
    "\\vdots \\\\\n",
    "Layer 2 \\\\\n",
    "\\vdots \\\\\n",
    "\\vdots \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Activations' shape is (n, attention_size)\n",
    "- Layer_2's shape is (attention_size, 1)\n",
    "\n",
    "### Calculate weights: Softmax function<a class=\"anchor\" id=\"2_1_5\"></a>\n",
    "$$\n",
    "\\large \\alpha_{ij} = \\frac{\\exp{\\left(e_{ij}\\right)}}{\\sum_{k=1}^K \\exp{\\left(e_{ik}\\right)}}\n",
    "$$\n",
    "\n",
    "### Apply the weights to the encoder's hidden states<a class=\"anchor\" id=\"2_1_6\"></a>\n",
    "$$\n",
    "\\large c_i = \\sum_{j=1}^K\\alpha_{ij} h_{j}\n",
    "$$\n",
    "\n",
    "### Code<a class=\"anchor\" id=\"2_1_7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b98a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "hidden_size = 16 # n\n",
    "attention_size = 10\n",
    "input_length = 5\n",
    "\n",
    "encoder_states = np.random.randn(input_length, hidden_size)\n",
    "decoder_state = np.random.randn(1, hidden_size)\n",
    "layer_1 = np.random.randn(2*hidden_size, attention_size)\n",
    "layer_2 = np.random.rand(attention_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab822bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=0):\n",
    "    \"\"\"    \n",
    "        axis=0 calculates softmax across rows which means each column sums to 1 \n",
    "        axis=1 calculates softmax across columns which means each row sums to 1\n",
    "    \"\"\"\n",
    "    return np.exp(x) / np.expand_dims(np.sum(np.exp(x), axis=axis), axis)\n",
    "\n",
    "def alignment(encoder_states, decoder_state):\n",
    "    inputs = np.concatenate((encoder_states, np.repeat(decoder_state, input_length, axis=0)), axis=1)\n",
    "    activations = np.tanh(np.dot(inputs, layer_1))\n",
    "    scores = np.dot(activations, layer_2)\n",
    "    return scores\n",
    "\n",
    "def activations(encoder_states, decoder_state):\n",
    "    scores = alignment(encoder_states, decoder_state)\n",
    "    weights = softmax(scores)\n",
    "    weighted_scores = encoder_states * weights\n",
    "    context = np.sum(weighted_scores, axis=0)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f4332",
   "metadata": {},
   "source": [
    "## Scaled dot product attention:<a class=\"anchor\" id=\"2_2\"></a>\n",
    "2017's paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) introduced a different type of attention that doesn't use neural networks but only matrix product to calculate the weights to scale the inputs. However, this method uses aligned vector spaces, and that needs to be computed before the scaled dot product attention is calculated. \n",
    "\n",
    "It is based on three matrices, Q (Queries), K (keys) and V (Values). \n",
    "- The queries are the inputs\n",
    "- The keys are the outputs\n",
    "- And the values are the inputs as well. See the formula, where they are the values to weight.\n",
    "\n",
    "The queries and keys matrices are used to calculate alignment scores that measure how well the queries and keys match. Again, a measure of similarity.\n",
    "\n",
    "$$ \n",
    "\\large \\mathrm{Attention}\\left(Q, K, V\\right) = \\mathrm{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "- $Q路K^T$ is a measure of similarity between $Q$ and $K$. \n",
    "- Scaled down by a regularization constant $sqrt(d_k)$, with $d_k$ being the dimension of the keys.\n",
    "- The softmax calculates weights\n",
    "- These weights are then used to weight $V$ (usually, the inputs)\n",
    "\n",
    "In the context of machine translation, the word embeddings, in two different languages would have been aligned separately, using other algorighms that transform the embeddings to a common space so that words with similar meanings in say en and Fre are closer together once aligned. \n",
    "\n",
    "One property of alignment is that they will relate words (assign higher scores for that pair) even if the words appear at different indexes in the input and the output. This should come from the fact that $Q$ and $K$ are matrices containing all input and output vectors. Therefore the dot product of $Q$ and $K$ will calculate a measure of similarity for all pair of input and output vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a32d14",
   "metadata": {},
   "source": [
    "### Code<a class=\"anchor\" id=\"2_2_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6e8c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(queries, keys):\n",
    "    dot = np.matmul(queries, keys.T)/np.sqrt(keys.shape[1])\n",
    "    weights = softmax(dot, axis=1)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def attention_qkv(queries, keys, values):\n",
    "    return np.matmul(calculate_weights(queries, keys), values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff078a7",
   "metadata": {},
   "source": [
    "### Visualizing how the attention relates similar words from inputs and outputs<a class=\"anchor\" id=\"2_2_2\"></a>\n",
    "\n",
    "#### Aligned vectors <a class=\"anchor\" id=\"2_2_2_1\"></a>\n",
    "As mentioend above, scaled dot product attention assumes that aligned vectors have already been calculated by other means. Below and in the context of Neural Machine Translation (NMT) we use vocabulary dictionaries and embedding matrices. They are pre-computed and related to each other, that is, the vocabulary dictionaries map a word key to an integer index, which is the row index of this word in the embeddings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load dictionaries: word -> row index of word in embeddigs matrix\n",
    "with open(\"./data/word2int_en.pkl\", \"rb\") as f:\n",
    "    en_words = pickle.load(f)\n",
    "    \n",
    "with open(\"./data/word2int_fr.pkl\", \"rb\") as f:\n",
    "    fr_words = pickle.load(f)\n",
    "\n",
    "# Load the word embeddings: embedding at index i is related to the word dictionaries\n",
    "en_embeddings = np.load(\"./data/embeddings_en.npz\")[\"embeddings\"]\n",
    "fr_embeddings = np.load(\"./data/embeddings_fr.npz\")[\"embeddings\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1960aed2",
   "metadata": {},
   "source": [
    "#### Calculating embedding matrices for our concrete inputs and outputs <a class=\"anchor\" id=\"2_2_2_2\"></a>\n",
    "In the context of (NMT), the inputs of the attention layer are matrices of embedding aligned vectors; one vector per word in the input and expected output. So we need to calculate these embedding matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b4c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence, token_mapping):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "        - sentence: str\n",
    "        - token_mapping: a dictionary to map from word to int index.\n",
    "    Output\n",
    "        - An array of indexes for each word in the input\n",
    "    \"\"\"\n",
    "    tokenized = []\n",
    "    \n",
    "    for word in sentence.lower().split(\" \"):\n",
    "        try:\n",
    "            tokenized.append(token_mapping[word])\n",
    "        except KeyError:\n",
    "            # Using -1 to indicate an unknown word\n",
    "            tokenized.append(-1)\n",
    "        \n",
    "    return tokenized\n",
    "\n",
    "def embed(tokens, embeddings):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "        - tokens: an array of indexes\n",
    "        - embeddings: a matrix of shape (vocab_size, emb_dim) containing row word embeddings\n",
    "    Output\n",
    "        - A matrix with shape (len(tokens), emb_dim) with the word emeddings for the tokens in 'tokens'\n",
    "    \"\"\"\n",
    "    embed_size = embeddings.shape[1]\n",
    "    \n",
    "    output = np.zeros((len(tokens), embed_size))\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == -1:\n",
    "            output[i] = np.zeros((1, embed_size))\n",
    "        else:\n",
    "            output[i] = embeddings[token]\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25efe3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize example sentences in English and French, then get their embeddings\n",
    "sentence_en = \"The agreement on the European Economic Area was signed in August 1992 .\"\n",
    "tokenized_en = tokenize(sentence_en, en_words)\n",
    "embedded_en = embed(tokenized_en, en_embeddings)\n",
    "\n",
    "sentence_fr = \"L accord sur la zone 茅conomique europ茅enne a 茅t茅 sign茅 en ao没t 1992 .\"\n",
    "tokenized_fr = tokenize(sentence_fr, fr_words)\n",
    "embedded_fr = embed(tokenized_fr, fr_embeddings)\n",
    "\n",
    "# These weights indicate alignment between words in English and French\n",
    "alignment = calculate_weights(embedded_fr, embedded_en)\n",
    "\n",
    "# Visualize weights to check for alignment\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.imshow(alignment, cmap='inferno')\n",
    "ax.xaxis.tick_top()\n",
    "ax.set_xticks(np.arange(alignment.shape[1]))\n",
    "ax.set_xticklabels(sentence_en.split(\" \"), rotation=90, size=16);\n",
    "ax.set_yticks(np.arange(alignment.shape[0]));\n",
    "ax.set_yticklabels(sentence_fr.split(\" \"), size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e8a358",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0ccdd54",
   "metadata": {},
   "source": [
    "Note how the correct translations are highlighted (assigned higher scores). This is because the embedding vectors in En and Fr were already aligned. But also, notice how 'sign茅' and 'agreement' (and the reciprocal 'accord' and 'signed') receive an also high score, again, because in the aligned space, those two embedding vectors are close together. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_abs-text-sum-project)",
   "language": "python",
   "name": "conda_abs-text-sum-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
