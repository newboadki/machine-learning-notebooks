{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "This notebook fine-tunes the t5-small model checkpoint on a small subset of Reviews.csv. \n",
    "This is related to abstractive-text-summarization-project notebook, but I included it as a separate notebook because the dependencies and requirements are substantially different, PyTorch, HuggingFace transformers and it requires cuda acceleration to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers rouge-score nltk torch -q\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_dataset(*, path, nrows, train_split, test_split=0.1, test_val_split=0.5):\n",
    "    \"\"\"\n",
    "    Creates a HF DataSet from a csv file and splits it into train and test using \n",
    "    the given parameters.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, nrows=nrows)\n",
    "    \n",
    "    # Keep only properties of interest\n",
    "    df = df[['Text','Summary']]\n",
    "\n",
    "    # Rename\n",
    "    df.rename(columns={\"Summary\": \"summary\", \"Text\": \"document\"}, inplace=True)\n",
    "    df.drop_duplicates(keep='first')\n",
    "    df = df.dropna()\n",
    "    df['document']= df['document'].apply(lambda x: x.lower())\n",
    "    df['summary'] = df['summary'].apply(lambda x: x.lower())\n",
    "\n",
    "    # Convert to HF DataSet\n",
    "    raw_datasets = Dataset.from_pandas(df)\n",
    "    raw_datasets = raw_datasets.rename_column(\"__index_level_0__\", \"id\")\n",
    "    \n",
    "    # Split into train, validation and test sets\n",
    "    raw_datasets = raw_datasets.train_test_split(train_size=train_split, test_size=test_split)\n",
    "    test_valid = raw_datasets['test'].train_test_split(test_size=test_val_split)\n",
    "\n",
    "    train_test_valid_dataset = DatasetDict({\n",
    "        'train': raw_datasets['train'],\n",
    "        'test': test_valid['test'],\n",
    "        'valid': test_valid['train']})\n",
    "\n",
    "    return train_test_valid_dataset\n",
    "        \n",
    "dataset = processed_dataset(path='/kaggle/input/reviews/Reviews.csv',\n",
    "                            nrows=100000,\n",
    "                            train_split=0.05,\n",
    "                            test_split=0.1, \n",
    "                            test_val_split=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "- We will use the t5-small checkpoint\n",
    "- This section was tested using a cuda enabled machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "model_checkpoint ='t5-small'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
    "max_input_length = 1024\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "- The input was lowercased in *processed_dataset*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "def preprocess_function(examples, max_input_length=1024, max_target_length=128):\n",
    "    inputs = ['summarize:' + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs,\n",
    "                             max_length=max_input_length,\n",
    "                             truncation=True,\n",
    "                             padding='max_length')\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"],\n",
    "                           max_length=max_target_length,\n",
    "                           truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "train = dataset['train']\n",
    "valid = dataset['valid']\n",
    "test = dataset['test']\n",
    "tokenized_train = train.map(preprocess_function, batched=True)\n",
    "tokenized_valid = valid.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning\n",
    "## Preparing the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "batch_size = 16\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the device we will be using for training\n",
    "print(\"[INFO] training using {}\".format(torch.cuda.get_device_name(0)))\n",
    "print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "torch.cuda.empty_cache()\n",
    "%env WANDB_DISABLED=True\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-amazon-fine-goods-reviews\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metric = load_metric(\"rouge\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying your own samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import pipeline\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "index= 6\n",
    "sample_text = valid[index]['document']\n",
    "sample_summ = valid[index]['summary']\n",
    "\n",
    "pipe = pipeline(\"summarization\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                device=0)\n",
    "pipe_out = pipe(sample_text)\n",
    "print(pipe_out[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QutitdB8C9S2"
   },
   "source": [
    "# Metrics and results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we fine-tuned on 5000 samples from Reviews.csv, the model achieves a Rouge1 score of 14.89, significantly higher than the values obtained by training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/t5-small-training-eval.PNG\" alt=\"Training results and metrics\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, these scores are not close to 50-60, which are considered state of the art. Some of the reasons are:\n",
    "- We used 5000 of 500_000 samples available for fine-tuning.\n",
    "- The pre-trained model is clearly more verbose than the target summaries from Reviews.csv dataset, and the target summaries contain lots of short titles that summarize the article in a very suscint way, with words that might not appear in the article. Since Rouge is recall based (looking for overlapping from the reference into the candidates), this can partially justify lower scores.\n",
    "\n",
    "## Some examples\n",
    "\n",
    "**Article 1**\n",
    "this is my favorite tea of all time. i drink many different varieties including teavana and other loose leaf gourmet teas and teabags, and this one is still my favorite. it is great to drink all year and not just during the holidays.\n",
    "\n",
    "**Generated summary**\n",
    "my favorite tea of all time - and not just during the holidays - i love this tea. i'm a big fan of this tea\n",
    "\n",
    "\n",
    "**Article 2**\n",
    "shipping was very fast, but the product is so so. many other reviews compared these to slim jims in one way or another,so here is my thoughts. these are alot drier and a whole lot less greaser than slim jims, which is 2 good things about them, but i didn`t care for the flavor or after taste they leave you with. i rate these average at best, but will keep them for a quick snack now and then. along with my order i also recieved 2 samples; maple pepper beef jerky & buffalo bills \"moist & tender\" beef jerky, both are very tasty and a product i will buy again. i still plan on trying more of choo choo snacks other products in the future.\n",
    "\n",
    "**Generated summary**\n",
    "alot drier and a lot less greaser than slim jims, but i didn't care for the flavor or after taste\n",
    "\n",
    "\n",
    "\n",
    "**Article 3**\n",
    "we are from europe so our chocolate / christmas candy tastes differs from many in america.  but here is the scoop on this candy.  plum jam infused with liquor is wrapped in marzipan, (a paste made out of finally ground almonds, sugar and cream).  the whole thing is dipped in dark chocolate. this is an exquisite treat, pricy yet worth it, this one time a year.  it is best nibbled and savored slowly.  if you have kids, you can let them have a few bites or even a whole 1 piece.  the alcohol content is not too high.  however, if you intend to scarf up the whole box by yourself, you may spend some time happily sitting under the christmas tree.  ok, ok, so it's only my friends and i who live under there,  (since it is our european costume to hang these and other boozy christmas chocolates on the tree as part of the decorations).  so, give it a whirl!  try a box and enjoy!\n",
    "\n",
    "**Generated summary**\n",
    "a whirl of chocolate / christmas candy - if you have kids, you may spend some time under the christmas tree!\n",
    "\n",
    "\n",
    "**Article 4**\n",
    "i'm glad to find this again. it's tea with some zip added in. taste good with some lemon and ethier hot or cold. good product at a good price!(and much cheaper than my local health food store)\n",
    "\n",
    "**Generated summary**\n",
    "good tea! good price. good product. good price! good product & good quality. good value! ! :-) good product!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
