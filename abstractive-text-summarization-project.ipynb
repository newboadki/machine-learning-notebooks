{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization\n",
    "\n",
    "## Abstract\n",
    "The goal of this notebook is to implement, measure and compare different ML models for abstractive text summarisation.\n",
    "\n",
    "In particular, three versions will be explored: two recurrent-based networks and a transformer-based network.\n",
    "- LSTM\n",
    "- Bi-directional GRU\n",
    "- Fine tuning a pre-trained transformer model\n",
    "\n",
    "## Introduction\n",
    "The objective is to generate an abstractive summary of a given text input. Abstractive in this context means that potentially new words will be generated to represent the original input. This is in contrast with extractive text summarization strategies which would take parts of the original input and compose them to generate the summary.\n",
    "\n",
    "## Table of contents\n",
    "* [1. Framing the problem](#framing-the-problem)\n",
    "* [2. The data](#the-data)\n",
    "* [3. Exploring the data](#exploring-the-data)\n",
    "* [4. Processing the data](#processing-the-data)\n",
    "* [5. Model exploration](#model-exploration)\n",
    "    * [5.1 LSTM-based architecture](#lstm)\n",
    "    * [5.2 GRU-based architecture](#gru)\n",
    "* [6. Comparing the results](#comparing-the-results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Framing the problem <a class=\"anchor\" id=\"framing-the-problem\"></a>\n",
    "\n",
    "The business objective is to produce a shorter version of an input text containing the most important bits of information in order to process more documents without loosing the substance of the original text.\n",
    "\n",
    "We can express this problem more formally as follows. The abstractive text summarization algorithm receives a sequence of words (s1, s2, ..., sn) and returns another sequence (o1, o2, ..., om), where m < n and the elements in the output sequence do not need to appear in the input sequence.\n",
    "\n",
    "This can be solved using supervised learning where pairs of text and their manually generated summaries will be provided to the learning algorithm. \n",
    "\n",
    "The models to explore will be the traditional seqToSeq recurrent neural networks and pretrained transformer all of which will use an encoder-decoder architecture.\n",
    "\n",
    "We will use TensorFlow and Hugging Face Transformers to create the neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Data <a class=\"anchor\" id=\"the-data\"></a>\n",
    "We will use Amazon's fine goods data set, which contains 500_000 samples. However, we will only use 100_000 samples for training and validation due to the limited computational resources available.\n",
    "\n",
    "Amazon's Fine Goods Reviews data set from Kaggle\n",
    "https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews\n",
    "\n",
    "## 3. Exploring the Data <a class=\"anchor\" id=\"exploring-the-data\"></a>\n",
    "In the following section we will:\n",
    "- Check the structure of the data set\n",
    "- Display only Text and Summary columns since that's all we are interested in\n",
    "- Observe from the previous point that the summaries are quite short. This will limit the size of the generated summaries, which might be a problem in the business problem required longer documents and summaries. We could at this point find another data set.\n",
    "- Plot a chart to confirm the observation about the length of the summaries, with most of them being under 10 words long. And this is before we process the data, which will slightly shorten the summaries, since we can see\n",
    "that most of them use a very suscint language with few articles, which are one type of words that will be\n",
    "removed during preprocessing\n",
    "\n",
    "Additional checks we could perform including checking for rare words, since we might want to remove them during the processing of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas\n",
    "# pip install numpy\n",
    "# pip install seaborn\n",
    "# pip install matplotlib\n",
    "# pip install nltk\n",
    "# pip install worldcloud\n",
    "# pip install scikit-learn\n",
    "# conda install -c apple tensorflow-deps\n",
    "# pip install tensorflow-macos\n",
    "# pip install tensorflow-metal\n",
    "# pip install -q contractions==0.0.48\n",
    "# Download TF_text (https://github.com/sun1638650145/Libraries-and-Extensions-for-TensorFlow-for-Apple-Silicon/releases/download/v2.13/tensorflow_text-2.13.0-cp38-cp38-macosx_11_0_arm64.whl)\n",
    "# pip install <path-to-TF-text.whl>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foundation\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import string\n",
    "import unicodedata\n",
    "from random import randint\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Data sets\n",
    "from dataset_utils import merge_headlines_and_text_from_csv_files, percentage_of_words_that_count_under_limit\n",
    "from nlp_preprocessing import preprocess_data_frame, remove_indexes, preprocess_text, remove_long_text_and_summary_from_data_frame\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from attention import AttentionLayer\n",
    "from tf_utils import DataSets, tokenized_padded_sets, get_embedding_matrix\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# # TensorFlow\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding, TimeDistributed, Concatenate\n",
    "from keras import backend as K \n",
    "\n",
    "import nltk\n",
    "\n",
    "from summary_inference import inference_models_from_lstm, decode_sequence, seq2summary, seq2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set and print structure\n",
    "reviews_path = \"./data/Reviews.csv\"\n",
    "df = pd.read_csv(reviews_path, nrows=100000)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/exploring-the-data-1.PNG\" alt=\"Initial data structure\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display only the relevant columns\n",
    "df.loc[:,['Text', 'Summary']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/exploring-the-data-2.PNG\" alt=\"Initial data structure\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's cuantify the size of the summaries in the training set\n",
    "summary_lengths = [len(summary.split()) for summary in df.Summary if isinstance(summary, str)]\n",
    "\n",
    "pd.DataFrame({'summaries': summary_lengths}).hist(bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/exploring-the-data-3.PNG\" alt=\"Initial data structure\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Processing the data <a class=\"anchor\" id=\"processing-the-data\"></a>\n",
    "- Leaving only the Text and Summary columns\n",
    "- Rename to text and headlines. This is purely for consistency if we decide to use another data set.\n",
    "- Remove duplicates\n",
    "- Lower case\n",
    "- Remove html tags\n",
    "- Remove hyperlinks\n",
    "- Expand contractions\n",
    "- Remove possessive apostrophe\n",
    "- Remove numbers\n",
    "- Remove stopwords\n",
    "- Remove special characters\n",
    "- Remove entries with a length not within the max limits that we'll determine after further data inspection\n",
    "- Add start and end tokens to the summaries\n",
    "- Shuffle the data\n",
    "\n",
    "In this example we will preprocess the text and summary in the same way. Except for adding the start and end tags to the summaries.\n",
    "\n",
    "The implementation of all these utilities can be found in _nlp_preprocessing.py_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == DATA SET ==\n",
    "# Amazon's Fine Goods Reviews data set from Kaggle\n",
    "# https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews\n",
    "# The data set contains 500.000 samples. Here we are training with a subset to speed up the training\n",
    "start_token = 'tokenstart'\n",
    "end_token = 'tokenend'\n",
    "\n",
    "def remove_columns_except_headlines_and_text(data_frame):\n",
    "    cols_to_remove = data_frame.columns.tolist()\n",
    "    cols_to_remove.remove('Summary')\n",
    "    cols_to_remove.remove('Text')\n",
    "\n",
    "    data_frame.drop(cols_to_remove, axis='columns', inplace=True)\n",
    "\n",
    "remove_columns_except_headlines_and_text(df)\n",
    "df.rename(columns={\"Summary\": \"headlines\", \"Text\": \"text\"}, inplace=True)\n",
    "print(f\"Before removing duplicates {len(df)}\")\n",
    "\n",
    "df.drop_duplicates(subset=['text'], inplace=True)\n",
    "df.dropna(axis=0, inplace=True)\n",
    "print(f\"After removing duplicates {len(df)}\")\n",
    "\n",
    "preprocess_data_frame(df, stemming=False, start_token=start_token, end_token=end_token)\n",
    "# df = shuffle(df)\n",
    "print(\"Finished preprocessing the data set.\")\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/processing-the-data-1.PNG\" alt=\"Initial data structure\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETERMINING MAX-LENGTH OF PROCESSED SENTENCES\n",
    "#\n",
    "# Since we haven't particularly checked the variability of the length of the sentences\n",
    "# nor the presence of outliers, we will graphically represent the distribution of length counts\n",
    "# and we will pick a value for max-length that is close to the minimum value that\n",
    "# leaves below a large % of the population.\n",
    "\n",
    "text_count = [len(sentence.split()) for sentence in df.text]\n",
    "headlines_count = [len(sentence.split()) for sentence in df.headlines]\n",
    "\n",
    "pd.DataFrame({'text': text_count, 'headlines': headlines_count}).hist(bins=30)\n",
    "plt.show()\n",
    "\n",
    "print(f\"The real max length of in the text column is {np.max(text_count)}\")\n",
    "print(f\"The real max length of in the headlines column is {np.max(headlines_count)}\")\n",
    "\n",
    "text_ratio_70 = percentage_of_words_that_count_under_limit(df.text, 70)\n",
    "text_ratio_80 = percentage_of_words_that_count_under_limit(df.text, 80)\n",
    "text_ratio_90 = percentage_of_words_that_count_under_limit(df.text, 90)\n",
    "text_ratio_100 = percentage_of_words_that_count_under_limit(df.text, 100)\n",
    "headline_ratio_10 = percentage_of_words_that_count_under_limit(df.headlines, 10)\n",
    "headline_ratio_12 = percentage_of_words_that_count_under_limit(df.headlines, 12)\n",
    "\n",
    "print(f\"Ratio of text with a count equal or less than {70} is {text_ratio_70}\")\n",
    "print(f\"Ratio of text with a count equal or less than {80} is {text_ratio_80}\")\n",
    "print(f\"Ratio of text with a count equal or less than {90} is {text_ratio_90}\")\n",
    "print(f\"Ratio of text with a count equal or less than {100} is {text_ratio_100}\")\n",
    "\n",
    "print(f\"Ratio of headlines with a count equal or less than {10} is {headline_ratio_10}\")\n",
    "print(f\"Ratio of headlines with a count equal or less than {12} is {headline_ratio_12}\")\n",
    "\n",
    "\n",
    "# Therefore we will pick:\n",
    "# 100 as the text's max-length since 94% of the population has 100 or less words\n",
    "# 10 as the text's max-length since 99% of the population has 10 or less words\n",
    "max_text_len = 100\n",
    "max_summary_len = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/processing-the-data-2.PNG\" alt=\"Initial data structure\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_long_text_and_summary_from_data_frame(df, \n",
    "                                             max_text_len, \n",
    "                                             max_summary_len, \n",
    "                                             \"text\", \n",
    "                                             \"headlines\")\n",
    "print(f'Dataset size: {len(df)} after deleting long text and summaries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data sets, Tokenize and padd the sequences\n",
    "x_tokenizer = Tokenizer()\n",
    "y_tokenizer = Tokenizer()\n",
    "\n",
    "data_sets = tokenized_padded_sets(df, x_tokenizer, y_tokenizer, max_text_len, max_summary_len, test_size_ratio=0.1)\n",
    "\n",
    "x_train_padded = data_sets.train.x\n",
    "y_train_padded = data_sets.train.y\n",
    "x_val_padded = data_sets.val.x\n",
    "y_val_padded = data_sets.val.y\n",
    "\n",
    "# Remove summary and texts where the summary only has sostok & eostok\n",
    "remove_train_indexes = remove_indexes(y_train_padded)\n",
    "remove_val_indexes = remove_indexes(y_val_padded)\n",
    "\n",
    "y_train_padded = np.delete(y_train_padded, remove_train_indexes, axis=0)\n",
    "x_train_padded = np.delete(x_train_padded, remove_train_indexes, axis=0)\n",
    "y_val_padded = np.delete(y_val_padded, remove_val_indexes, axis=0)\n",
    "x_val_padded = np.delete(x_val_padded, remove_val_indexes, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Exploration <a class=\"anchor\" id=\"model-exploration\"></a>\n",
    "\n",
    "### 5.1 ) LSTM-based network <a class=\"anchor\" id=\"lstm\"></a>\n",
    "\n",
    "Below is the diagram of the first model. It follows an encoder-decoder architecture. \n",
    "\n",
    "The encoder has three LSTM cells. For each cell we display the time steps.\n",
    "\n",
    "The attention layer sits between the encoder and the decoder, capturing the encoder's hidden states into a context vector that is passed to the decoder.\n",
    "\n",
    "The decoder contains a single LSTM layer, that receives the attention layer's output and the decoder's last step's output. The decoder also has a dense and a softmax layer to create a probability distribution of what the next word will be. The example below implements a greedy strategy by simply selecting the most probable next word.\n",
    "\n",
    "<img src=\"./images/lstm-based-model-arch.PNG\" alt=\"LSTM-based architecture\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == LSTM-based network ==\n",
    "\n",
    "def enc_dec_with_att_model(latent_dim, max_text_len):\n",
    "    K.clear_session()\n",
    "\n",
    "    #-------------------------------\n",
    "    # Encoder\n",
    "    #-------------------------------\n",
    "    encoder_inputs = Input(shape=(max_text_len,))\n",
    "    enc_emb = Embedding(data_sets.x_vocab_size, latent_dim, trainable=True)(encoder_inputs)\n",
    "\n",
    "    #LSTM 1\n",
    "    encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "    #LSTM 2\n",
    "    encoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "    #LSTM 3\n",
    "    encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True)\n",
    "    encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "    #-------------------------------\n",
    "    # Decoder\n",
    "    #-------------------------------\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    dec_emb_layer = Embedding(data_sets.y_vocab_size, latent_dim, trainable=True)\n",
    "    dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "    # LSTM using encoder_states as initial state\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "    #-------------------------------\n",
    "    # Attention Layer\n",
    "    #-------------------------------\n",
    "    attn_layer = AttentionLayer(name='attention_layer')\n",
    "    attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "    # Concat attention output and decoder LSTM output\n",
    "    decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "    #-------------------------------\n",
    "    # Dense layer\n",
    "    #-------------------------------\n",
    "    decoder_dense = TimeDistributed(Dense(data_sets.y_vocab_size, activation='softmax'))\n",
    "    decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'encoder_inputs': encoder_inputs,\n",
    "        'encoder_outputs': encoder_outputs, \n",
    "        'decoder_inputs': decoder_inputs,\n",
    "        'decoder_outputs': decoder_outputs,\n",
    "        'state_h': state_h,\n",
    "        'state_c': state_c,\n",
    "        'dec_emb_layer': dec_emb_layer,\n",
    "        'decoder_lstm': decoder_lstm,\n",
    "        'attn_layer': attn_layer,\n",
    "        'decoder_dense': decoder_dense\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('amazon-reviews-model.h5')\n",
    "\n",
    "# # LOADING THE MODEL\n",
    "# savedModel = tf.keras.models.load_model(\n",
    "#        'amazon-reviews-model-25092023.h5',\n",
    "#        custom_objects={'AttentionLayer':AttentionLayer}\n",
    "# )\n",
    "\n",
    "\n",
    "# savedModel.summary()\n",
    "# model_info['model'] = savedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == MODEL TRAINING == \n",
    "\n",
    "latent_dim = 256\n",
    "\n",
    "def train_lstm_model(latent_dim):\n",
    "    model_info = enc_dec_with_att_model(latent_dim, max_text_len)\n",
    "    model = model_info['model']\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    batch_size = 512\n",
    "    epochs = 50\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit([x_train_padded, y_train_padded[:,:-1]], y_train_padded.reshape(y_train_padded.shape[0], y_train_padded.shape[1], 1)[:,1:], epochs=epochs, callbacks=[es], batch_size=batch_size, validation_data=([x_val_padded, y_val_padded[:,:-1]], y_val_padded.reshape(y_val_padded.shape[0],y_val_padded.shape[1], 1)[:,1:]))\n",
    "    \n",
    "    return model_info, history\n",
    "    \n",
    "    \n",
    "lstm_model_info, lstm_history = train_lstm_model(latent_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/lstm-1.PNG\" alt=\"Initial data structure\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == DRAWING THE ACCURACY == \n",
    "plt.plot(lstm_history.history['accuracy'][1:], label='train acc')\n",
    "plt.plot(lstm_history.history['val_accuracy'], label='val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == DRAWING THE LOSS == \n",
    "from matplotlib import pyplot \n",
    "\n",
    "pyplot.plot(lstm_history.history['loss'], label='train-val') \n",
    "pyplot.plot(lstm_history.history['val_loss'], label='test-val') \n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM-based model Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_encoder_model, inf_decoder_model = inference_models_from_lstm(lstm_model_info, latent_dim, max_text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index = y_tokenizer.index_word \n",
    "reverse_source_word_index = x_tokenizer.index_word \n",
    "target_word_index = y_tokenizer.word_index\n",
    "\n",
    "summary_labels = []\n",
    "summary_preds = []\n",
    "\n",
    "for i in range(10):\n",
    "  original_summary = seq2summary(y_val_padded[i], target_word_index, reverse_target_word_index)\n",
    "  prediction = decode_sequence(x_val_padded[i].reshape(1, max_text_len), max_summary_len, inf_encoder_model, inf_decoder_model, target_word_index, reverse_target_word_index)\n",
    "  summary_labels.append(original_summary)\n",
    "  summary_preds.append(prediction)\n",
    "\n",
    "print(\"Done generating predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model_info['model'].save('lstm-amazon-reviews-031020230554.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-directional GRU-based network <a class=\"anchor\" id=\"gru\"></a>\n",
    "\n",
    "The second model also follows an encoder-decoder architecture. \n",
    "\n",
    "The encoder has two bidirectional GRU cells.\n",
    "\n",
    "The same attention and decoder layers are used.\n",
    "\n",
    "<img src=\"./images/lstm-based-model-arch.PNG\" alt=\"LSTM-based architecture\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import GRU, Embedding, Dropout\n",
    "\n",
    "\n",
    "latent_dimension = 256\n",
    "embedding_dim = 100\n",
    "x_embedding_matrix = get_embedding_matrix(x_tokenizer, embedding_dim, data_sets.x_vocab_size)\n",
    "y_embedding_matrix = get_embedding_matrix(y_tokenizer, embedding_dim, data_sets.y_vocab_size)\n",
    "\n",
    "\n",
    "def enc_dec_with_att_model_bi_gru(latent_dim, \n",
    "                                  max_text_len, \n",
    "                                  embedding_dim, \n",
    "                                  x_vocab_size, \n",
    "                                  y_vocab_size,\n",
    "                                  x_embedding_matrix, \n",
    "                                  y_embedding_matrix):\n",
    "    encoder_input = Input(shape=(max_text_len, ))\n",
    "    decoder_input = Input(shape=(None, ))\n",
    "\n",
    "    # ENCODER\n",
    "    encoder_embedding = Embedding(x_vocab_size,\n",
    "                                  embedding_dim,\n",
    "                                  embeddings_initializer=Constant(x_embedding_matrix),\n",
    "                                  trainable=False)(encoder_input) \n",
    "\n",
    "    # GRU 1\n",
    "    encoder_gru_01 = Bidirectional(GRU(latent_dim, return_sequences=True, return_state=True))\n",
    "    encoder_output_01, encoder_forward_state_01, encoder_backward_state_01 = encoder_gru_01(encoder_embedding)\n",
    "    encoder_output_dropout_01 = Dropout(0.3)(encoder_output_01)\n",
    "\n",
    "    # GRU 2\n",
    "    encoder_gru_02 = Bidirectional(GRU(latent_dim, return_sequences=True, return_state=True))\n",
    "    encoder_output, encoder_forward_state, encoder_backward_state = encoder_gru_02(encoder_output_dropout_01)\n",
    "    encoder_state = Concatenate()([encoder_forward_state, encoder_backward_state])\n",
    "\n",
    "    # DECODER\n",
    "    decoder_embedding_layer = Embedding(y_vocab_size,\n",
    "                                  embedding_dim,\n",
    "                                  embeddings_initializer=Constant(y_embedding_matrix),\n",
    "                                  trainable=False)\n",
    "    decoder_embedding = decoder_embedding_layer(decoder_input)\n",
    "\n",
    "    # GRU using encoder_states as initial state\n",
    "    decoder_gru = GRU(latent_dim*2, return_sequences=True, return_state=True)\n",
    "    decoder_output, decoder_state = decoder_gru(decoder_embedding, initial_state=[encoder_state])\n",
    "\n",
    "    # Attention Layer\n",
    "    attention_layer = AttentionLayer() \n",
    "    attention_out, attention_states = attention_layer([encoder_output, decoder_output])\n",
    "\n",
    "    # Concat attention output and decoder GRU output \n",
    "    decoder_concatenate = Concatenate(axis=-1)([decoder_output, attention_out])\n",
    "\n",
    "    # Dense layer\n",
    "    decoder_dense = TimeDistributed(Dense(y_vocab_size, activation='softmax')) #hierarchical\n",
    "    decoder_dense_output = decoder_dense(decoder_concatenate)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model([encoder_input, decoder_input], decoder_dense_output)\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'encoder_input': encoder_input,\n",
    "        'encoder_output': encoder_output, \n",
    "        'encoder_state': encoder_state,\n",
    "        'decoder_input': decoder_input,\n",
    "        'decoder_state': decoder_state,\n",
    "        'decoder_dense': decoder_dense,\n",
    "        'decoder_embedding_layer': decoder_embedding_layer,\n",
    "        'decoder_gru': decoder_gru\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Bi-GRU MODEL TRAINING == \n",
    "\n",
    "latent_dim = 256\n",
    "\n",
    "def train_gru_model(latent_dim, \n",
    "                   max_text_len, \n",
    "                   embedding_dim, \n",
    "                   x_vocab_size,\n",
    "                   y_vocab_size,\n",
    "                   x_embedding_matrix, \n",
    "                   y_embedding_matrix):\n",
    "    model_info = enc_dec_with_att_model_bi_gru(latent_dim, \n",
    "                                        max_text_len, \n",
    "                                        embedding_dim, \n",
    "                                        x_vocab_size,\n",
    "                                        y_vocab_size,\n",
    "                                        x_embedding_matrix, \n",
    "                                        y_embedding_matrix)\n",
    "    model = model_info['model']\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    batch_size = 512\n",
    "    epochs = 50\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit([x_train_padded, y_train_padded[:,:-1]], y_train_padded.reshape(y_train_padded.shape[0], y_train_padded.shape[1], 1)[:,1:], epochs=epochs, callbacks=[es], batch_size=batch_size, validation_data=([x_val_padded, y_val_padded[:,:-1]], y_val_padded.reshape(y_val_padded.shape[0],y_val_padded.shape[1], 1)[:,1:]))\n",
    "    \n",
    "    return model_info, history\n",
    "    \n",
    "    \n",
    "gru_model_info, gru_history = train_gru_model(latent_dim, \n",
    "                   max_text_len, \n",
    "                   embedding_dim, \n",
    "                   data_sets.x_vocab_size,\n",
    "                   data_sets.y_vocab_size,\n",
    "                   x_embedding_matrix, \n",
    "                   y_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model_info['model'].save('gru-amazon-reviews-031020230708.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == DRAWING THE ACCURACY == \n",
    "plt.plot(gru_history.history['accuracy'][1:], label='train acc')\n",
    "plt.plot(gru_history.history['val_accuracy'], label='val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == DRAWING THE LOSS == \n",
    "from matplotlib import pyplot \n",
    "\n",
    "pyplot.plot(gru_history.history['loss'], label='train-val') \n",
    "pyplot.plot(gru_history.history['val_loss'], label='test-val') \n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_based_inference_models(model_info, latent_dim, max_text_len):\n",
    "    model = model_info['model']\n",
    "    encoder_input = model_info['encoder_input']\n",
    "    encoder_output = model_info['encoder_output']\n",
    "    encoder_state = model_info['encoder_state']\n",
    "    decoder_input = model_info['decoder_input']\n",
    "    decoder_state = model_info['decoder_state']\n",
    "    decoder_dense = model_info['decoder_dense']\n",
    "    y_embedding_layer = model_info['decoder_embedding_layer']\n",
    "    decoder_gru = model_info['decoder_gru']\n",
    "    \n",
    "    # Encoder Inference Model\n",
    "    encoder_model_inference = Model(encoder_input, [encoder_output, encoder_state])\n",
    "\n",
    "    # Decoder Inference\n",
    "    # Below tensors will hold the states of the previous time step\n",
    "    decoder_state = Input(shape=(latent_dim*2, ))\n",
    "    decoder_intermittent_state_input = Input(shape=(max_text_len, latent_dim*2))\n",
    "\n",
    "    # Get Embeddings of Decoder Sequence\n",
    "    decoder_embedding_inference = y_embedding_layer(decoder_input)\n",
    "\n",
    "    # Predict Next Word in Sequence, Set Initial State to State from Previous Time Step\n",
    "    decoder_output_inference, decoder_state_inference = decoder_gru(decoder_embedding_inference,\n",
    "                                                                    initial_state=[decoder_state])\n",
    "\n",
    "    # Attention Inference\n",
    "    attention_layer = AttentionLayer()\n",
    "    attention_out_inference, attention_state_inference = attention_layer([decoder_intermittent_state_input,\n",
    "                                                                          decoder_output_inference])\n",
    "    decoder_inference_concat = Concatenate(axis=-1)([decoder_output_inference,\n",
    "                                                     attention_out_inference])\n",
    "\n",
    "    # Dense Softmax Layer to Generate Prob. Dist. Over Target Vocabulary\n",
    "    decoder_output_inference = decoder_dense(decoder_inference_concat)\n",
    "\n",
    "    # Final Decoder Model\n",
    "    decoder_model_inference = Model([decoder_input, decoder_intermittent_state_input, decoder_state], \n",
    "                                    [decoder_output_inference, decoder_state_inference])\n",
    "    \n",
    "    return encoder_model_inference, decoder_model_inference\n",
    "\n",
    "def decode_sequence(input_sequence,\n",
    "                    max_summary_len,\n",
    "                    enc_inference_model, \n",
    "                    dec_inference_model, \n",
    "                    start_token, \n",
    "                    end_token, \n",
    "                    target_word_index,\n",
    "                    reverse_target_word_index):\n",
    "  \"\"\"Text generation function via encoder / decoder network.\"\"\"\n",
    "\n",
    "  # Encode Input as State Vectors.\n",
    "  encoder_output, encoder_state = enc_inference_model.predict(input_sequence)\n",
    "\n",
    "  # Generate Empty Target Sequence of Length 1.\n",
    "  target_sequence = np.zeros((1, 1))\n",
    "\n",
    "  # Choose 'start' as the first word of the target sequence\n",
    "  target_sequence[0, 0] = target_word_index[start_token]\n",
    "\n",
    "  decoded_sentence = ''\n",
    "  break_condition = False\n",
    "  while not break_condition:\n",
    "      token_output, decoder_state = dec_inference_model.predict([target_sequence, \n",
    "                                                                 encoder_output,\n",
    "                                                                 encoder_state])\n",
    "\n",
    "      # Sample Token\n",
    "      sampled_token_index = np.argmax(token_output[0, -1, :])\n",
    "\n",
    "      if not sampled_token_index == 0:\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "\n",
    "        if not sampled_token == end_token:\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "\n",
    "        # Break Condition: Encounter Max Length / Find Stop Token.\n",
    "        if sampled_token == end_token or len(decoded_sentence.split()) >= (max_summary_len - 1):\n",
    "            break_condition = True\n",
    "\n",
    "        # Update Target Sequence (length 1).\n",
    "        target_sequence = np.zeros((1, 1))\n",
    "        target_sequence[0, 0] = sampled_token_index\n",
    "\n",
    "      else:\n",
    "        break_condition = True\n",
    "\n",
    "      # Update internal states\n",
    "      encoder_state = decoder_state\n",
    "\n",
    "  return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_gru_inf_model, dec_gru_inf_model = gru_based_inference_models(gru_model_info,\n",
    "                                                                  latent_dim, max_text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index = y_tokenizer.index_word \n",
    "reverse_source_word_index = x_tokenizer.index_word \n",
    "target_word_index = y_tokenizer.word_index\n",
    "\n",
    "summary_labels = []\n",
    "summary_preds = []\n",
    "\n",
    "for i in range(100, 150):\n",
    "  original_summary = seq2summary(y_val_padded[i], target_word_index, reverse_target_word_index)\n",
    "  prediction = decode_sequence(x_val_padded[i].reshape(1, max_text_len),\n",
    "                               max_summary_len,\n",
    "                               enc_gru_inf_model,\n",
    "                               dec_gru_inf_model,\n",
    "                               start_token,\n",
    "                               end_token,\n",
    "                               target_word_index,\n",
    "                               reverse_target_word_index)\n",
    "  summary_labels.append(original_summary)\n",
    "  summary_preds.append(prediction)\n",
    "\n",
    "print(\"Done generating predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, summaries in enumerate(zip(summary_labels, summary_preds)):\n",
    "    input_seq = x_val_padded[index]\n",
    "    original_text = seq2text(input_seq, reverse_source_word_index)\n",
    "    print(f\"original text: {original_text}\")\n",
    "    print(f\"Original: {summaries[0]}\")\n",
    "    print(f\"Prediction: {summaries[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "summary_labels_tf = tf.ragged.constant([nltk.word_tokenize(s) for s in summary_labels])\n",
    "summary_preds_tf = tf.ragged.constant([nltk.word_tokenize(s) for s in summary_preds])\n",
    "result = text.metrics.rouge_l(summary_preds_tf, summary_labels_tf)\n",
    "\n",
    "print('F-Measure: %s' % result.f_measure)\n",
    "print('P-Measure: %s' % result.p_measure)\n",
    "print('R-Measure: %s' % result.r_measure)\n",
    "\n",
    "print(np.sum(result.f_measure)/50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow_text's ROGUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "summary_labels_tf = tf.ragged.constant([nltk.word_tokenize(s) for s in summary_labels])\n",
    "summary_preds_tf = tf.ragged.constant([nltk.word_tokenize(s) for s in summary_preds])\n",
    "result = text.metrics.rouge_l(summary_preds_tf, summary_labels_tf)\n",
    "\n",
    "print('F-Measure: %s' % result.f_measure)\n",
    "print('P-Measure: %s' % result.p_measure)\n",
    "print('R-Measure: %s' % result.r_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras' ROGUE\n",
    "Below is another API to calculate ROUGE scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install keral_nlp\n",
    "# pip install rouge-score\n",
    "import keras_nlp\n",
    "import rouge_score\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "\n",
    "rouge_l = keras_nlp.metrics.RougeL()\n",
    "rouge_l(summary_labels, summary_preds)[\"f1_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying your own inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = [preprocess_text(\"bought several vitality canned dog food products found ththem good quality product looks like stew processed meat smells better labrador finicky appreciates product better\")]\n",
    "print(type(thing[0]))\n",
    "my_review = np.array(thing)\n",
    "print(type(my_review))\n",
    "print(my_review)\n",
    "my_tokenizer = Tokenizer()\n",
    "my_tokenizer.fit_on_texts(my_review) # creates a dictionary of symbols for each word\n",
    "\n",
    "# Convert text sequences into integer sequences\n",
    "tokenized_sequence = x_tokenizer.texts_to_sequences(my_review) \n",
    "\n",
    "# == PADDING == zero upto maximum length\n",
    "padded_tokenized_sequence = pad_sequences(tokenized_sequence,  maxlen=max_text_len, padding='post') \n",
    "\n",
    "summary = decode_sequence(padded_tokenized_sequence, max_summary_len, inf_encoder_model, inf_decoder_model, target_word_index, reverse_target_word_index)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Comparing the results <a class=\"anchor\" id=\"comparing-the-results\"></a>\n",
    "\n",
    "We will look into manual human evaluation of the results and the computed ROUGE metrics.\n",
    "\n",
    "Given the small size of the training set, both types of Recurrent neural network take a considerably long time to train and their results won't take our breath away. Nevertheless, they don't produce gibberish and what they both produce tend to actually summarize the sentiment of the source text, yet in a very abstracted and condensed form. \n",
    "\n",
    "## Manual evaluation\n",
    "\n",
    "When inspecting the results manually\n",
    "- The LSTM architecture took twice as much to train, and produces summaries that are a lot more repetitive and with less information content. It also got some results completely wrong. \n",
    "- The GRU is considerably better overall and takes less time to train, so it opens the door for further improvement iterations, increasing the training sample size and potentially increasing the complexity of the network to capture more information.\n",
    "\n",
    "## ROUGE metrics\n",
    "After manually inspecting the results we can expect the ROUGE scores to be quite low, and they are. This is because the summaries in the training set are quite short and the score is based on number of occurrences of words from the source appearing in the target and viceversa. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyOcwqfZ+siexQyqy0CWNsHG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Environment (conda_abs-text-sum-project)",
   "language": "python",
   "name": "conda_abs-text-sum-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
